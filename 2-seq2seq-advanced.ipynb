{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced dynamic seq2seq with TensorFlow\n",
    "\n",
    "# 用tensorflow实现的高级动态seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder is bidirectional now. Decoder is implemented using `tf.nn.raw_rnn`. It feeds previously generated tokens during training as inputs, instead of target sequence.\n",
    "\n",
    "编码器现在是双向的。解码器使用`tf.nn.raw_rnn`实现。 它在训练期间将以前生成的tokens作为输入，而不是目标序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regular seq2seq 常规seq2seq\n",
    "\n",
    "![seq2seq architecutre](pictures/1-seq2seq.png)\n",
    "\n",
    "Rectangles are encoder and decoder's recurrent layers. Encoder receives `[A, B, C]` sequence as inputs. We don't care about encoder outputs, only about the hidden state it accumulates while reading the sequence. After input sequence ends, encoder passes its final state to decoder, which receives `[<EOS>, W, X, Y, Z]` and is trained to output `[W, X, Y, Z, <EOS>]`. `<EOS>` token is a special word in vocabulary that signals to decoder the beginning of translation.\n",
    "\n",
    "矩形是编码器和解码器的循环层。 编码器接收`[A，B，C]`序列作为输入。 我们不关心编码器输出，只关于读取序列时积累的隐藏状态。在输入序列结束后，编码器将其最终状态传递给解码器，解码器接收`[<EOS>，W，X，Y，Z]`，并经过训练以输出`[W，X，Y，Z，<EOS>]`。 `<EOS>`token是词汇中的一个特殊单词，用于指示解码器的翻译开始。\n",
    "\n",
    "## Implementation details 实现细节\n",
    "\n",
    "TensorFlow has its own [implementation of seq2seq](https://www.tensorflow.org/tutorials/seq2seq/). Recently it was moved from core examples to [`tensorflow/models` repo](https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate), and uses deprecated seq2seq implementation. Deprecation happened because it uses **static unrolling**. \n",
    "\n",
    "tensorflow有它自己的 [seq2seq实现](https://www.tensorflow.org/tutorials/seq2seq/)。最近它转移到了核心例程  [`tensorflow/models` repo](https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate)，并使用不推荐使用的seq2seq实现，因为使用 **静态展开**，所以不推荐使用。\n",
    "\n",
    "**Static unrolling** involves construction of computation graph with a fixed sequence of time step. Such a graph can only handle sequences of specific lengths. One solution for handling sequences of varying lengths is to create multiple graphs with different time lengths and separate the dataset into this buckets.\n",
    "\n",
    "**静态展开** 涉及用固定的时间步长构建计算图。 这样的图形只能处理特定长度的序列。 而处理不同长度的序列的解决方案是创建具有不同时间长度的多个图形，并将数据集分成此存储桶。\n",
    "\n",
    "**Dynamic unrolling** instead uses control flow ops to process sequence step by step. In TF this is supposed to more space efficient and just as fast. This is now a recommended way to implement RNNs.\n",
    "\n",
    "**动态展开** 代替使用控制流操作来逐步处理序列。 在TF中，这应该具有更高的空间效率，同样快。 现在是实施RNN的推荐方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-56c6c727b0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m#matrix math\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m \u001b[1;31m#machine learningt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;31m#for formatting data into batches and generating random sequence data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "#we give encoder input sequence like 'hello how are you', we take the last hidden state and feed to decoder and it\n",
    "#will generate a decoded value. we compare that to target value, if translation would be 'bonjour ca va' and minimize \n",
    "#the difference by optimizing a loss function\n",
    "# 我们给编码器输入序列像'hello how are you'，我们把最后一个隐藏的状态提供给解码器，它会产生一个解码的值。 \n",
    "# 我们将其与目标值进行比较，如果翻译将是‘bonjour ca va’，并通过优化损失函数来最小化差异\n",
    "\n",
    "#in this case we just want to encode and decode the input successfully\n",
    "# 在这种情况下，我们只想对输入进行编码和解码\n",
    "\n",
    "#bidirectional encoder 双向编码器\n",
    "#We will teach our model to memorize and reproduce input sequence. \n",
    "#Sequences will be random, with varying length.\n",
    "#Since random sequences do not contain any structure, model will not be able to exploit any patterns in data. \n",
    "#It will simply encode sequence in a thought vector, then decode from it.\n",
    "#this is not about prediction (end goal), it's about understanding this architecture\n",
    "#我们将教我们的模型来记忆和再现输入序列。\n",
    "#Sequences将是随机的，长度不同。\n",
    "#因为随机序列不包含任何结构，模型将无法利用数据中的任何模式。\n",
    "#它将简单地在一个思想向量中编码序列，然后从它解码。\n",
    "#这不是关于预测（最终目标），而是关于理解这个架构\n",
    "\n",
    "#this is an encoder-decoder architecture. The encoder is bidrectional so \n",
    "#it It feeds previously generated tokens during training as inputs, instead of target sequence.\n",
    "#这是一个编码器 - 解码器架构。 编码器是双向的，因此它在训练期间将以前生成的token作为输入，而不是目标序列。\n",
    "\n",
    "import numpy as np #matrix math \n",
    "import tensorflow as tf #machine learningt\n",
    "import helpers #for formatting data into batches and generating random sequence data\n",
    "\n",
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First critical thing to decide: vocabulary size.\n",
    "#Dynamic RNN models can be adapted to different batch sizes \n",
    "#and sequence lengths without retraining \n",
    "#(e.g. by serializing model parameters and Graph definitions via tf.train.Saver), \n",
    "#but changing vocabulary size requires retraining the model.\n",
    "#决定第一关键事项：词汇大小。\n",
    "#动态RNN模型可以适应不同的批量大小和序列长度，而无需重新训练(例如通过串行化模型参数和通过tf.train.Saver绘制图形定义)\n",
    "#但是改变词汇大小需要重新训练模型。\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20 #character length\n",
    "\n",
    "encoder_hidden_units = 20 #num neurons\n",
    "decoder_hidden_units = encoder_hidden_units * 2 #in original paper, they used same number of neurons for both encoder\n",
    "#and decoder, but we use twice as many so decoded output is different, the target value is the original input \n",
    "#in this example\n",
    "# 在原始文件中，它们对于编码器和解码器都使用相同数量的神经元，但是我们使用的两倍的解码输出是不同的，目标值是本示例中的原始输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice way to understand complicated function is to study its signature - inputs and outputs. With pure functions, only inputs-output relation matters.\n",
    "\n",
    "理解复杂功能的好方法是研究其签名 - 输入和输出。 纯粹的功能来说，只有输入输出关系才是重要的。\n",
    "\n",
    "- `encoder_inputs` int32 tensor is shaped `[encoder_max_time, batch_size]`\n",
    "- `decoder_targets` int32 tensor is shaped `[decoder_max_time, batch_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we implement decoder with `tf.nn.raw_rnn` and will construct `decoder_inputs` step by step in the loop.\n",
    " \n",
    " 这里我们用`tf.nn.raw_rnn`实现解码器，并在循环中逐步构造`decode_inputs`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings 嵌入\n",
    "\n",
    "`encoder_inputs` and `decoder_inputs` are int32 tensors of shape `[max_time, batch_size]`, while encoder and decoder RNNs expect dense vector representation of words, `[max_time, batch_size, input_embedding_size]`. We convert one to another by using *word embeddings*. Specifics of working with embeddings are nicely described in [official tutorial on embeddings](https://www.tensorflow.org/tutorials/word2vec/).\n",
    "\n",
    "`encoder_inputs`和`decode_inputs`是形状为`[max_time，batch_size]`的int32张量，而编码器和解码器RNN则期望dense vector表示多个单词，`[max_time，batch_size，input_embedding_size]`。 我们通过使用 *字嵌入* 来转换另一个。 嵌入工作的具体内容在 [官方教程教程](https://www.tensorflow.org/tutorials/word2vec/) 中有很好的描述。\n",
    "\n",
    "First we initialize embedding matrix. Initializations are random. We rely on our end-to-end training to learn vector representations for words jointly with encoder and decoder.\n",
    "\n",
    "首先我们初始化嵌入矩阵。 初始化是随机的。 我们依靠我们的端到端训练来学习矢量表示在编码器和解码器中的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly initialized embedding matrrix that can fit input sequence\n",
    "#used to convert sequences to vectors (embeddings) for both encoder and decoder of the right size\n",
    "#reshaping is a thing, in TF you gotta make sure you tensors are the right shape (num dimensions)\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "#this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `tf.nn.embedding_lookup` to *index embedding matrix*: given word `4`, we represent it as 4th column of embedding matrix. \n",
    "This operation is lightweight, compared with alternative approach of one-hot encoding word `4` as `[0,0,0,1,0,0,0,0,0,0]` (vocab size 10) and then multiplying it by embedding matrix.\n",
    "\n",
    "我们使用`tf.nn.embedding_lookup`到 *索引嵌入矩阵*：给定的单词`4`，我们将其表示为第4列的嵌入矩阵。\n",
    "这个操作是轻量级的，与 One-Hot编码的“4”的替代方法相比，`[0,0,0,1,0,0,0,0,0,0]`(vocab size 10)，然后乘以 它通过嵌入矩阵。\n",
    "\n",
    "Additionally, we don't need to compute gradients for any columns except 4th.\n",
    "\n",
    "另外，除了4th之外，我们不需要计算任何列的渐变。\n",
    "\n",
    "In real NLP application embedding matrix can get very large, with 100k or even 1m columns.\n",
    "\n",
    "在实际的NLP应用中，嵌入矩阵可以获得非常大的，具有100k甚至1m的列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 编码器\n",
    "\n",
    "We are replacing unidirectional `tf.nn.dynamic_rnn` with `tf.nn.bidirectional_dynamic_rnn` as the encoder.\n",
    "\n",
    "我们用`tf.nn.bidirectional_dynamic_rnn`替代单向`tf.nn.dynamic_rnn`作为编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow.python.ops.rnn_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9fc07f841594>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTMCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTMStateTuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow.python.ops.rnn_cell'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.ops.rnn_cell import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x1182a8278>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable BiRNN_FW/LSTMCell/W_0 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-12-4a02fa6316cc>\", line 9, in <module>\n    dtype=tf.float32, time_major=True)\n  File \"/Users/sraval/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/sraval/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-614d82d179a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_inputs_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                     \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_inputs_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                     dtype=tf.float64, time_major=True)\n\u001b[0m\u001b[1;32m     10\u001b[0m     )\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[1;32m    664\u001b[0m   \u001b[0;31m# Backward direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1995\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars)\u001b[0m\n\u001b[1;32m   1877\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 1879\u001b[0;31m           pred, body, original_loop_vars, loop_vars)\n\u001b[0m\u001b[1;32m   1880\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 1829\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1830\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    997\u001b[0m           \u001b[0mcall_cell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcall_cell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m           \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m           skip_conditionals=True)\n\u001b[0m\u001b[1;32m   1000\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_rnn_step\u001b[0;34m(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;31m# steps.  This is faster when max_seq_len is equal to the number of unrolls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;31m# (which is typical for dynamic_rnn).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mnew_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    492\u001b[0m       concat_w = _get_concat_variable(\n\u001b[1;32m    493\u001b[0m           \u001b[0;34m\"W\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_proj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m           dtype, self._num_unit_shards)\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       b = vs.get_variable(\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell.py\u001b[0m in \u001b[0;36m_get_concat_variable\u001b[0;34m(name, shape, dtype, num_shards)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_concat_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   \u001b[0;34m\"\"\"Get a sharded variable concatenated into one tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m   \u001b[0msharded_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sharded_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharded_variable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msharded_variable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell.py\u001b[0m in \u001b[0;36m_get_sharded_variable\u001b[0;34m(name, shape, dtype, num_shards)\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0mcurrent_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     shards.append(vs.get_variable(name + \"_%d\" % i, [current_size] + shape[1:],\n\u001b[0;32m--> 358\u001b[0;31m                                   dtype=dtype))\n\u001b[0m\u001b[1;32m    359\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mshards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    871\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    698\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    215\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    200\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    492\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 494\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable BiRNN_FW/LSTMCell/W_0 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-12-4a02fa6316cc>\", line 9, in <module>\n    dtype=tf.float32, time_major=True)\n  File \"/Users/sraval/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/sraval/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "#get outputs and states\n",
    "#bidirectional RNN function takes a separate cell argument for \n",
    "#both the forward and backward RNN, and returns separate \n",
    "#outputs and states for both the forward and backward RNN\n",
    "\n",
    "#When using a standard RNN to make predictions we are only taking the “past” into account. \n",
    "#For certain tasks this makes sense (e.g. predicting the next word), but for some tasks \n",
    "#it would be useful to take both the past and the future into account. Think of a tagging task, \n",
    "#like part-of-speech tagging, where we want to assign a tag to each word in a sentence. \n",
    "#Here we already know the full sequence of words, and for each word we want to take not only the \n",
    "#words to the left (past) but also the words to the right (future) into account when making a prediction. \n",
    "#Bidirectional RNNs do exactly that. A bidirectional RNN is a combination of two RNNs – one runs forward from \n",
    "#“left to right” and one runs backward from “right to left”. These are commonly used for tagging tasks, or \n",
    "#when we want to embed a sequence into a fixed-length vector (beyond the scope of this post).\n",
    "\n",
    "\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float64, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to concatenate forward and backward outputs and state. In this case we will not discard outputs, they would be used for attention.\n",
    "\n",
    "必须连接前后输出和状态。 在这种情况下，我们不会丢弃输出，它们将被用于引起注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time and batch dimensions are dynamic, i.e. they can change in runtime, from batch to batch When decoding, feeding previously generated tokens as inputs adds robustness to model's errors. However feeding ground truth speeds up training. Apperantly best practice is to mix both randomly when training.\n",
    "\n",
    "时间和批量维度是动态的，即它们可以在运行时间内批量更改。当解码时，将以前生成的token作为输入，增加了模型错误的鲁棒性。 然而，喂养地面的真相加速了培训。 最佳练习是训练时随机混合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we could print this, won't need\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to decide how far to run decoder. There are several options for stopping criteria:\n",
    "\n",
    "接下来我们需要决定运行解码器多久。 有几个停止条件的选项：\n",
    "\n",
    "- Stop after specified number of unrolling steps  在具体数量的展开步骤后停止\n",
    "- Stop after model produced <EOS> token  模型生成后停止<EOS> token \n",
    "\n",
    "\n",
    "The choice will likely be time-dependant. In legacy `translate` tutorial we can see that decoder unrolls for `len(encoder_input)+10` to allow for possibly longer translated sequence. Here we are doing a toy copy task, so how about we unroll decoder for `len(encoder_input)+2`, to allow model some room to make mistakes over 2 additional steps \n",
    "\n",
    "这个选择很可能是时间依赖的。 在传统的 `翻译` 教程中，我们可以看到，解码器为 `len(encoder_input+10) 展开，以允许更长的翻译顺序。 在这里，我们正在做一个玩具复制任务，所以我们如何解开 `len（encoder_input+2` 的解码器，让模型有一些空间超过2个额外的步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output projection 输出投影\n",
    "\n",
    "Decoder will contain manually specified by us transition step: 解码器将由我们手动指定过渡步骤：\n",
    "```\n",
    "output(t) -> output projection(t) -> prediction(t) (argmax) -> input embedding(t+1) -> input(t+1)\n",
    "```\n",
    "\n",
    "In tutorial 1, we used `tf.contrib.layers.linear` layer to initialize weights and biases and apply operation for us. This is convenient, however now we need to specify parameters `W` and `b`  of the output layer in global scope, and apply them at every step of the decoder.\n",
    "\n",
    "在教程1中，我们使用`tf.contrib.layers.linear`层来初始化权重和偏差，并为我们应用操作。 这很方便，但是现在我们需要在全局范围内指定输出层的参数`W`和`b`，并在解码器的每个步骤中应用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder via `tf.nn.raw_rnn`\n",
    "\n",
    "`tf.nn.dynamic_rnn` allows for easy RNN construction, but is limited. \n",
    "\n",
    "`tf.nn.dynamic_rnn`可以方便RNN的构建，但是有限。\n",
    "\n",
    "For example, a nice way to increase robustness of the model is to feed as decoder inputs tokens that it previously generated, instead of shifted true sequence.\n",
    "\n",
    "例如，增加模型鲁棒性的好方法是作为先前产生的解码器输入token，而不是移动真实序列。\n",
    "\n",
    "![seq2seq-feed-previous](pictures/2-seq2seq-feed-previous.png)\n",
    "*Image borrowed from http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First prepare tokens. Decoder would operate on column vectors of shape `(batch_size,)` representing single time steps of the batch.\n",
    "\n",
    "首先准备token。 解码器将对表示批次的单个时间步长的形状`(batch_size,)`的列向量进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create padded inputs for the decoder from the word embeddings\n",
    "\n",
    "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the tricky part.\n",
    "\n",
    "现在是棘手的部分。\n",
    "\n",
    "Remember that standard `tf.nn.dynamic_rnn` requires all inputs `(t, ..., t+n)` be passed in advance as a single tensor. \"Dynamic\" part of its name refers to the fact that `n` can change from batch to batch.\n",
    "\n",
    "记住，标准`tf.nn.dynamic_rnn`需要所有输入`(t，...，t + n)`作为一个张量被提前传递。`动态`部分名称是指`n`可以从批次更改为事实。\n",
    "\n",
    "Now, what if we want to implement more complex mechanic like when we want decoder to receive previously generated tokens as input at every timestamp (instead of lagged target sequence)? Or when we want to implement soft attention, where at every timestep we add additional fixed-len representation, derived from query produced by previous step's hidden state? `tf.nn.raw_rnn` is a way to solve this problem.\n",
    "\n",
    "现在，如果我们想要实现更复杂的机制，就像我们希望解码器在每个时间戳（而不是滞后的目标序列）中接收先前生成的令牌作为输入的时候？或者当我们想要实现软件注意时，在每一个时间点，我们添加额外的固定表示，从上一步的隐藏状态产生的查询中派生？ `tf.nn.raw_rnn`是解决这个问题的一种方法。\n",
    "\n",
    "Main part of specifying RNN with `tf.nn.raw_rnn` is *loop transition function*. It defines inputs of step `t` given outputs and state of step `t-1`.\n",
    "\n",
    "用`tf.nn.raw_rnn`指定RNN的主要部分是*循环过渡功能*。它定义了给定输出步骤 `t` 和 输入和骤 `t-1`的状态。\n",
    "\n",
    "Loop transition function is a mapping `(time, previous_cell_output, previous_cell_state, previous_loop_state) -> (elements_finished, input, cell_state, output, loop_state)`. It is called *before* RNNCell to prepare its inputs and state. Everything is a Tensor except for initial call at time=0 when everything is `None` (except `time`).\n",
    "\n",
    "循环转换功能是一个映射`（time，previous_cell_output，previous_cell_state，previous_loop_state） - >（elements_finished，input，cell_state，output，loop_state）`。在* RNNCell之前被称为*准备其输入和状态。一切都是一个Tensor，除了初始调用时间= 0，当一切都是`None`（除了`时间`）。\n",
    "\n",
    "Note that decoder inputs are returned from the transition function but passed into it. You are supposed to index inputs manually using `time` Tensor.\n",
    "\n",
    "请注意，解码器输入从转换函数返回，但传递给它。您应该使用`time`Tensor手动索引输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop transition function is called two times:\n",
    " 1. Initial call at time=0 to provide initial cell_state and input to RNN.\n",
    " 2. Transition call for all following timesteps where you define transition between two adjacent steps.\n",
    "\n",
    "Lets define both cases separately.\n",
    "\n",
    "循环转化功能被调用了为两次：\n",
    "  1.在时间= 0的初始化调用，并提供初始的cell_state 和 输入到RNN。\n",
    "  2.转化调用所有以下时间步骤之间，您定义两个相邻步骤之间的转换。\n",
    "\n",
    "让我们分开定义这两种情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop initial state is function of only `encoder_final_state` and embeddings:\n",
    "\n",
    "循环初始状态只有 `encoder_final_state` 和嵌入的功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually specifying loop function through time - to get initial cell state and input to RNN\n",
    "#normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
    "\n",
    "#we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transition function such that previously generated token (as judged in greedy manner by `argmax` over output projection) is passed as next input.\n",
    "\n",
    "定义转换功能，使之以前生成的token（通过`argmax`通过输出投影以贪心的方式判断）作为下一个输入传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    \n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine initializer and transition functions and create raw_rnn.\n",
    "\n",
    "组合初始化和转换功能并创建raw_rnn。\n",
    "\n",
    "Note that while all operations above are defined with TF's control flow and reduction ops, here we rely on checking if state is `None` to determine if it is an initializer call or transition call. This is not very clean API and might be changed in the future (indeed, `tf.nn.raw_rnn`'s doc contains warning that API is experimental).\n",
    "\n",
    "请注意，虽然以上所有操作均由TF的控制流程和还原操作进行定义，但我们依赖于检查状态是否为`None`，以确定它是初始化程序调用还是转换调用。 这不是很干净的API，可能会在将来更改（确实，`tf.nn.raw_rnn`的文档包含API是实验性的警告）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do output projection, we have to temporarilly flatten `decoder_outputs` from `[max_steps, batch_size, hidden_dim]` to `[max_steps*batch_size, hidden_dim]`, as `tf.matmul` needs rank-2 tensors at most.\n",
    "\n",
    "要做输出投影，我们必须将 `decoder_outputs` 从 `[max_steps, batch_size, hidden_dim]` 到 `[max_steps*batch_size, hidden_dim]` 暂时平坦化，因为 `tf.matmul` 最多需要2级张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN outputs tensor of shape `[max_time, batch_size, hidden_units]` which projection layer maps onto `[max_time, batch_size, vocab_size]`. `vocab_size` part of the shape is static, while `max_time` and `batch_size` is dynamic.\n",
    "\n",
    "RNN输出 `[max_time，batch_size，hidden_units]` 形状的张量，这些投影层映射到 `[max_time，batch_size，vocab_size]` 上。 `vocab_size` 形状的一部分是静态的，而 `max_time` 和 `batch_size` 是动态的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the toy task 训练玩具任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the copy task — given a random sequence of integers from a `vocabulary`, learn to memorize and reproduce input sequence. Because sequences are random, they do not contain any structure, unlike natural language.\n",
    "\n",
    "考虑复制任务 - 给出一个来自 `词汇` 的随机的整数序列，学习记忆和再现输入序列。 因为序列是随机的，它们不包含任何结构，与自然语言不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[6, 5, 9, 4]\n",
      "[3, 5, 2, 8]\n",
      "[4, 9, 4]\n",
      "[7, 6, 7, 8, 4]\n",
      "[7, 7, 6, 7]\n",
      "[8, 9, 3]\n",
      "[2, 2, 7, 2]\n",
      "[5, 3, 8, 6, 8, 4, 7, 2]\n",
      "[5, 6, 2, 9, 7, 4, 5]\n",
      "[7, 3, 3, 9]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.2894203662872314\n",
      "  sample 1:\n",
      "    input     > [4 8 5 0 0 0 0 0]\n",
      "    predicted > [9 9 7 7 1 9 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 5 5 9 0 0 0 0]\n",
      "    predicted > [9 9 7 7 1 9 7 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 2 8 4 3 9 0 0]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.49065467715263367\n",
      "  sample 1:\n",
      "    input     > [3 7 8 9 2 6 4 0]\n",
      "    predicted > [3 7 9 8 2 4 4 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 8 2 8 7 0 0 0]\n",
      "    predicted > [5 8 2 8 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 4 9 2 0 0 0 0]\n",
      "    predicted > [5 4 9 2 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.2252199500799179\n",
      "  sample 1:\n",
      "    input     > [3 9 6 4 0 0 0 0]\n",
      "    predicted > [3 9 6 4 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 9 4 4 2 5 3 0]\n",
      "    predicted > [6 9 4 4 2 5 3 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 7 9 3 5 0 0 0]\n",
      "    predicted > [9 7 9 3 5 1 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.08593279868364334\n",
      "  sample 1:\n",
      "    input     > [3 2 7 2 8 0 0 0]\n",
      "    predicted > [3 2 7 2 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 5 7 5 9 7 5 0]\n",
      "    predicted > [6 5 7 5 9 7 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 2 5 2 0 0 0 0]\n",
      "    predicted > [4 2 5 2 1 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0869 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEoUU7iIiUUjhLiIShf4/5RW+zfyzYLEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c6a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
